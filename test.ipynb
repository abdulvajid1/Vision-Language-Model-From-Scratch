{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00f01490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple, List, Dict, Iterable, Union\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2d0edb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check device\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78006ff",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "297d02b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiglipVisionConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size=768,\n",
    "        intermediate_size=3072,\n",
    "        num_hidden_layers=12,\n",
    "        num_attention_heads=12,\n",
    "        num_channels=3,\n",
    "        image_size=224,\n",
    "        patch_size=16,\n",
    "        layer_norm_eps=1e-6,\n",
    "        attention_dropout=0.0,\n",
    "        num_image_tokens:int = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.num_channels = num_channels\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.num_image_tokens = num_image_tokens\n",
    "        \n",
    "config = SiglipVisionConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc448f0",
   "metadata": {},
   "source": [
    "## Visual Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d115ee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer taks a raw image -> patch_tokens with positional embeddings\n",
    "class SiglipPatchEmbedding(nn.Module):\n",
    "    def __init__(self, config: SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patch_layer = nn.Conv2d(in_channels=3,\n",
    "                                     out_channels=config.hidden_size,\n",
    "                                     kernel_size=config.patch_size,\n",
    "                                     stride=config.patch_size,\n",
    "                                     padding='valid')\n",
    "        \n",
    "        # total number of pixel in image = 224*224, total pixel in a patch is 16*16 , so total patch number = 224*224 / 16*16\n",
    "        self.patch_num = config.image_size**2 // config.patch_size**2\n",
    "        \n",
    "        self.pos_embedding = nn.Embedding(num_embeddings=self.patch_num,\n",
    "                                          embedding_dim=config.hidden_size)\n",
    "        \n",
    "        self.register_buffer('position_ids',\n",
    "                             torch.arange(0, self.patch_num).view(1, -1),\n",
    "                             persistent=False) # `persistent` = do we need it as a part of module state dict\n",
    "        \n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.patch_layer(x) # (Batch , channel, height, width) => (Batch, hidden_size, height, width)\n",
    "        # x = torch.flatten(x, start_dim=-2, end_dim=-1) => [Batch, hidden_size, heigt*widht = num_patch]\n",
    "        x = x.view(x.shape[0], x.shape[1], x.shape[2] * x.shape[3]) \n",
    "        # (Batch, num_patch, hidden_size)\n",
    "        x = x.transpose(-2, -1)\n",
    "        x = x + self.pos_embedding(self.position_ids)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.eps = config.layer_norm_eps\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.scale = nn.Parameter(torch.ones(self.hidden_size))\n",
    "        self.shift = nn.Parameter(torch.zeros(self.hidden_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_mean = torch.mean(x, dim=-1, keepdim=True)\n",
    "        x_std = torch.std(x, dim=-1, keepdim=True)\n",
    "        x_norm = (x - x_mean) / (x_std + self.eps)\n",
    "        return x_norm * self.scale + self.shift\n",
    "\n",
    "class SiglipAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.hidden_size = config.hidden_size\n",
    "        assert self.hidden_size % self.num_attention_heads == 0, \"hidden size should be divisible by num_attention_heads\"\n",
    "        self.attn_head_size = self.hidden_size // self.num_attention_heads\n",
    "        \n",
    "        self.q_W = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.k_W = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.v_W = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.out_W = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, patch_len, _ = x.shape\n",
    "        \n",
    "        queries = self.q_W(x)\n",
    "        keys = self.k_W(x)\n",
    "        values = self.v_W(x)\n",
    "        \n",
    "        # (Batch, patch, hidden_size) -> (Batch, patch, heads, head_embed) -> (batch, heads, patch, head_embed)\n",
    "        queries = queries.view(batch_size, patch_len, self.num_attention_heads, self.attn_head_size).transpose(1, 2)\n",
    "        keys = keys.view(batch_size, patch_len, self.num_attention_heads, self.attn_head_size).transpose(1, 2)\n",
    "        values = values.view(batch_size, patch_len, self.num_attention_heads, self.attn_head_size).transpose(1, 2)\n",
    "        \n",
    "        # attention_logits (b, head , patch, patch)\n",
    "        attention_logits = (queries @ keys.transpose(-1, -2)) * self.attn_head_size**-0.5\n",
    "        attention_scores = nn.functional.softmax(attention_logits, dim=-1, dtype=torch.float32)\n",
    "        \n",
    "        # (b, head, patch, patch) * (b, head, patch, embed) => (b, head, patch, embed)\n",
    "        contextual_embeddings = attention_scores @ values\n",
    "        \n",
    "        # (b, head, patch, embed) -> (b, patch, head, embed)\n",
    "        contextual_embeddings = contextual_embeddings.transpose(1, 2)\n",
    "        \n",
    "        contextual_embeddings = contextual_embeddings.contiguous().view(batch_size, patch_len, self.num_attention_heads*self.attn_head_size)\n",
    "        \n",
    "        contextual_embeddings = self.out_W(contextual_embeddings)\n",
    "        \n",
    "        return contextual_embeddings, attention_scores \n",
    "        \n",
    "\n",
    "\n",
    "class SiglipEncoderLayer(nn.Module):\n",
    "    def __init__(self, config: SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.layer_norm_1 = LayerNorm(config)\n",
    "        self.attention = SiglipAttention(config)\n",
    "        self.layer_norm_2 = LayerNorm(config)\n",
    "        self.final_linear = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.intermediate_size),\n",
    "            nn.GELU('tanh'),\n",
    "            nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        )        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual =  x\n",
    "        x = self.layer_norm_1(x)\n",
    "        x, _ = self.attention(x)\n",
    "        x = x + residual\n",
    "        \n",
    "        residual = x\n",
    "        x = self.layer_norm_2(x)\n",
    "        x = self.final_linear(x)\n",
    "        x = x + residual\n",
    "        \n",
    "        return x\n",
    "\n",
    "class SiglipEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_layer = config.num_hidden_layers\n",
    "        self.encoder_layers = nn.ModuleList(\n",
    "            [SiglipEncoderLayer(config) for _ in range(self.n_layer)]\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "class SiglipVisionTransformer(nn.Module):\n",
    "    def __init__(self, config: SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.patch_embedding = SiglipPatchEmbedding(config)\n",
    "        self.encoder = SiglipEncoder(config) \n",
    "        self.post_layer_norm = LayerNorm(config)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # (b, channel, height, width) -> (b, patch_len, patch_embedding)\n",
    "        x = self.patch_embedding(x)\n",
    "        # (b, num_patch, embedding_size) same as before\n",
    "        x = self.encoder(x) \n",
    "        x = self.post_layer_norm(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "        \n",
    "class SiglipVisionModel(nn.Module):\n",
    "    def __init__(self, config: SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.visual_model = SiglipVisionTransformer(config)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # [Batch, channel, height, width] -> [Batch, patch, embed_dim]\n",
    "        return self.visual_model(x)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d197a7c8",
   "metadata": {},
   "source": [
    "## Siglip Input Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6071a767",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_STANDARD_MEAN = [0.5, 0.5, 0.5]\n",
    "IMAGENET_STANDARD_STD = [0.5, 0.5, 0.5]\n",
    "\n",
    "def resize(image: Image,\n",
    "           size: Tuple[int, int],\n",
    "           resample: Image.Resampling=None,\n",
    "           reducing_gap: Optional[int]=None):\n",
    "    height, width = size\n",
    "    resize_image = image.resize(\n",
    "        (width, height), resample=resample, reducing_gap=reducing_gap\n",
    "    )\n",
    "    \n",
    "    return resize_image\n",
    "\n",
    "def rescale(image: np.ndarray,\n",
    "            scale: float,\n",
    "            dtype: np.dtype = np.float32):\n",
    "    rescaled_image = image * scale\n",
    "    rescaled_image = rescaled_image.astype(dtype)\n",
    "    return rescaled_image\n",
    "\n",
    "def normalize(image: Image.Image,\n",
    "              mean: Union[float, Iterable[float]],\n",
    "              std: Union[float, Iterable[float]],\n",
    "              ):\n",
    "    mean = np.array(mean)\n",
    "    std = np.array(std)\n",
    "    normalized_image = (image - mean) / std\n",
    "    \n",
    "    return normalized_image\n",
    "    \n",
    "\n",
    "def process_image(\n",
    "    images: List[Image.Image],\n",
    "    size: Dict[str, int] = None,\n",
    "    resample: Image.Resampling = None,\n",
    "    rescale_factor: float=None,\n",
    "    image_mean: Optional[Union[float, List[float]]] = None,\n",
    "    image_std: Optional[Union[float, List[float]]] = None,\n",
    "):\n",
    "    height, width = size[0], size[1]\n",
    "    \n",
    "    images = [\n",
    "        resize(image=image, size=(height, width), resample=resample) for image in images\n",
    "    ]\n",
    "    \n",
    "    images = [np.array(image) for image in images]\n",
    "    \n",
    "    images = [rescale(image, scale=rescale_factor) for image in images]\n",
    "    \n",
    "    images = [normalize(image, mean=image_mean, std=image_std) for image in images]\n",
    "    \n",
    "    images = [image.transpose(2, 0, 1) for image in images]\n",
    "    \n",
    "    return images\n",
    "\n",
    "def add_image_tokens_to_prompt(prefix_prompt,\n",
    "                               bos_token,\n",
    "                               image_seq_len,\n",
    "                               image_token):\n",
    "    \n",
    "    return f\"{image_token * image_seq_len}{bos_token}{prefix_prompt}\\n\"\n",
    "    \n",
    "    \n",
    "\n",
    "class PaliGemmaProcessor:\n",
    "    \n",
    "    IMAGE_TOKEN = \"<image>\"\n",
    "    \n",
    "    def __init__(self, tokenizer, num_image_token: int, image_size: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.image_seq_len = num_image_token # num_patches\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        # Special token to add\n",
    "        token_to_add = {'additional_special_token': [self.IMAGE_TOKEN]}\n",
    "        tokenizer.add_special_tokens(token_to_add) # added special token in hugging face tokenizer\n",
    "        \n",
    "        EXTRA_TOKENS = [\n",
    "            f\"<loc{i:04d}>\" for i in range(1024)\n",
    "        ]\n",
    "        \n",
    "        EXTRA_TOKENS += [\n",
    "            f'<seg{i:03d}>' for i in range(128)\n",
    "        ]\n",
    "        \n",
    "        tokenizer.add_tokens(EXTRA_TOKENS)\n",
    "        \n",
    "        self.image_token_id = tokenizer.convert_tokens_to_ids(self.IMAGE_TOKEN)\n",
    "        \n",
    "        tokenizer.add_bos_token = False\n",
    "        tokenizer.add_eos_token = False\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __call__(self, \n",
    "                 texts: List[str],\n",
    "                 images: List[Image.Image],\n",
    "                 padding=False,\n",
    "                 truncate=False) -> dict:\n",
    "        \n",
    "        assert len(images) == len(texts), \"the number of text and image should be same\"\n",
    "        \n",
    "        pixel_values = process_image(\n",
    "            images,\n",
    "            size=(self.image_size, self.image_size),\n",
    "            resample=Image.Resampling.BICUBIC,\n",
    "            rescale_factor=1 / 255.0,\n",
    "            image_mean=IMAGENET_STANDARD_MEAN,\n",
    "            image_std=IMAGENET_STANDARD_STD\n",
    "        )\n",
    "        \n",
    "        pixel_values = np.stack(pixel_values, axis=0)\n",
    "        pixel_values = torch.tensor(pixel_values)\n",
    "        \n",
    "        input_string = [\n",
    "            add_image_tokens_to_prompt(\n",
    "                prefix_prompt=prompt,\n",
    "                bos_token=self.tokenizer.bos_token,\n",
    "                image_seq_len=self.image_seq_len,\n",
    "                image_token=self.IMAGE_TOKEN\n",
    "            )\n",
    "            for prompt in texts\n",
    "        ]\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            input_string,\n",
    "            return_tesors='pt',\n",
    "            padding=padding,\n",
    "            truncation=truncate,\n",
    "        )\n",
    "        \n",
    "        return {'pixel_values': pixel_values, **inputs} # input_ids and attention mask\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb934bb",
   "metadata": {},
   "source": [
    "## Langauge Model - Gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f4c8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaConfig:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 hidden_size,\n",
    "                 intermediate_size,\n",
    "                 num_hidden_layers,\n",
    "                 num_attention_heads,\n",
    "                 num_key_value_heads,\n",
    "                 head_dim=256,\n",
    "                 max_position_embeddings=2048,\n",
    "                 rms_norm_eps=1e-6,\n",
    "                 rope_theta=100000.0,\n",
    "                 attention_bias=False,\n",
    "                 attention_dropout=0.0,\n",
    "                 pad_token_id=None,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.num_key_value_heads = num_key_value_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.rms_norm_eps = rms_norm_eps\n",
    "        self.rope_theta = rope_theta\n",
    "        self.attention_bias = attention_bias\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.pad_token_id = pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c8a891",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PaligemmaConfig():\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vision_config=None,\n",
    "        text_config=None,\n",
    "        ignore_index=-100,\n",
    "        image_token_index=256000,\n",
    "        vocab_size=257152,\n",
    "        projection_dim=2048,\n",
    "        hidden_size=2048,\n",
    "        pad_token_id = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.vision_config = SiglipVisionConfig(**vision_config)\n",
    "        self.text_config = GemmaConfig(**text_config)\n",
    "        self.is_encoder_decoder = False\n",
    "        self.ignore_index = ignore_index\n",
    "        self.image_token_index = image_token_index\n",
    "        self.projection_dim = projection_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.pad_token_id = pad_token_id \n",
    "        self.vocab_size = self.text_config.vocab_size\n",
    "        \n",
    "        self.text_config.num_image_tokens = self.vision_config.image_size**2 // self.vision_config.patch_size**2\n",
    "        self.vision_config.projection_dim = self.projection_dim\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5261b64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d733e14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaliGemmaForConditionalGeneration(nn.Module):\n",
    "    def __init__(self, config: PaliGemmaConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        self.vision_model = SiglipVisionModel(config.vision_config)\n",
    "        self.multi_model_projector = PaliGemmaMultiModelProjector(config) # A Linear projector that standardize text and image embedding size\n",
    "        self.vocab_size = config.vacab_size\n",
    "        \n",
    "        self.language_model = GemmaForCasualLm(config.text_config)\n",
    "        \n",
    "        self.pad_token = self.config.pad_token_id if self.confg.pad_token_id is not -1\n",
    "        \n",
    "    # share the weights of embedding (vocab_size * embdding_size) to output projecting layer weights (embedding_size * vocab_size)\n",
    "    def tie_weights(self):\n",
    "        return self.language_model.tie_weights()\n",
    "    \n",
    "    def _merge_input_ids_with_image_features(\n",
    "        self, image_features: torch.Tensor,\n",
    "        input_embeds: torch.Tensor,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        kv_cache\n",
    "    ):\n",
    "        embed_dim = image_features.shape[-1]\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        dtype, device = input_embeds.dtype, input_embeds.device\n",
    "        \n",
    "        # same scaling as in the attention for same reason, for consistent magnitude event the number parameter increase (different size of models)\n",
    "        scaled_image_features = image_features / (self.config.hidden_size**0.5)\n",
    "        final_embedding = torch.zeros(batch_size, seq_len, embed_dim, dtype=dtype, device=device)\n",
    "        \n",
    "        text_mask = (input_ids != self.config.image_token_index) & (input_ids != self.pad_token_id)\n",
    "        image_mask = (input_ids == self.config.image_token_index)\n",
    "        pad_mask = (input_ids == self.pad_token_id)\n",
    "        \n",
    "        text_mask_expanded = text_mask.unsqueeze(-1).expand(-1, -1, embed_dim)\n",
    "        image_mask_expanded = text_mask.unsqueeze(-1).expand(-1, -1, embed_dim)\n",
    "        pad_mask_expanded = text_mask.unsqueeze(-1).expand(-1, -1, embed_dim)\n",
    "        \n",
    "        # final_embedding (b, total_seq_len, embed_size)\n",
    "        # all mask = (b, total_seq_len, embed_size )\n",
    "        # image_feature (b, patch_len, embed)\n",
    "        final_embedding = torch.where(text_mask_expanded, input_embeds, final_embedding)\n",
    "        # we use masked_scatter since our scale_image_feature is different shape than final multimodel input shape, so torch.where gives error, but masked_scatter do the same thing in different way, you can learn it \n",
    "        final_embedding = torch.masked_scatter(final_embedding, image_mask_expanded, scaled_image_features)\n",
    "        final_embedding = torch.where(pad_mask_expanded, torch.zeros_like(final_embedding), final_embedding)\n",
    "        \n",
    "        # KV CACHE\n",
    "        min_dtype = torch.finfo(dtype=dtype)\n",
    "        q_len = input_embeds.shape[1]\n",
    "        \n",
    "        if kv_cache is None or kv_cache.num_items() == 1:\n",
    "            # Prefilling phase\n",
    "            casual_mask =  torch.full(size=(batch_size, q_len, q_len),\n",
    "                                      fill_value=0,\n",
    "                                      dtype=dtype,\n",
    "                                      device=device)\n",
    "        else:\n",
    "            \n",
    "            assert q_len == 1, \"when enable kv cache the query len should be one\"\n",
    "            \n",
    "            kv_len = kv_cache.num_items() + q_len\n",
    "            \n",
    "            casual_mask = torch.full(size=(batch_size, q_len, kv_len),\n",
    "                                      fill_value=0,\n",
    "                                      dtype=dtype,\n",
    "                                      device=device)\n",
    "            \n",
    "            # (batch, query_len, kv_len) -> (batch, att_head, query_len, kv_len``)\n",
    "            casual_mask = casual_mask.unsqueeze(1) # Adding a head dimensions since we will have multiple head in attention\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        def forward(\n",
    "            self,\n",
    "            input_ids:torch.LongTensor=None,\n",
    "            pixel_values:torch.FloatTensor=None,\n",
    "            attention_mask:Optional[torch.Tensor]=None,\n",
    "            kv_cache=Optional[KVCache]=None\n",
    "        ) -> Tuple:\n",
    "            assert torch.all(attention_mask==1), \"The input can't padded\" # for this implementation only\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def forward(self,\n",
    "                input_ids: torch.LongTensor = None,\n",
    "                pixel_values: torch.FloatTensor = None,\n",
    "                attention_mask: Optional[torch.Tensor] = None,\n",
    "                kv_cache: Optional[KVCache] = None) -> Tuple :\n",
    "        \n",
    "        assert torch.all(attention_mask == 1), \"The input can't be padded\"\n",
    "        \n",
    "        # tokens ids to embeddings [this input embedding contain embedding of image place holder token that need to replace by original image embedding]\n",
    "        input_embeddings = self.language_model.get_input_embeddings()(input_ids) # (B, Seq_len, embedding_size)\n",
    "        \n",
    "        # extract visual feature (b, patch_len, embedding )\n",
    "        visual_embeddings = self.vision_model(pixel_values)\n",
    "        \n",
    "        # project visual embedding to a standard embedding size as text embedding\n",
    "        visual_embeddings = self.multi_model_projector(visual_embeddings) # (b, patch_len, patch_embed) -> (b, patch_len, d_model) for merge between text and image embedding\n",
    "        \n",
    "        # replace image placeholder token embedding with real image embedding\n",
    "        input_embeddings, attention_mask, position_ids =  self._merge_input_ids_with_image_features(visual_embeddings, input_embeddings, input_ids, attention_mask, kv_cache)\n",
    "        \n",
    "        outputs = self.language_model(\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            input_embeddings=input_embeddings,\n",
    "            kv_cache=kv_cache\n",
    "        )\n",
    "        return outputs\n",
    "               \n",
    "        \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d5c8ac0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'nan_items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m10\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16)\u001b[38;5;241m.\u001b[39mnan_items()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'nan_items'"
     ]
    }
   ],
   "source": [
    "torch.rand(10, dtype=torch.float16).nan_items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cc94ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]]], device='mps:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.full(size=(3, 2, 2), fill_value=0, dtype=torch.float16, device='mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f4b114",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
