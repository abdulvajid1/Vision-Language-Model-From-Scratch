{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f01490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple, List, Dict, Iterable, Union\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2d0edb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check device\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "297d02b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiglipVisionConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size=768,\n",
    "        intermediate_size=3072,\n",
    "        num_hidden_layers=12,\n",
    "        num_attention_heads=12,\n",
    "        num_channels=3,\n",
    "        image_size=224,\n",
    "        patch_size=16,\n",
    "        layer_norm_eps=1e-6,\n",
    "        attention_dropout=0.0,\n",
    "        num_image_tokens:int = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.num_channels = num_channels\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.num_image_tokens = num_image_tokens\n",
    "        \n",
    "config = SiglipVisionConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "000a341b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiglipPatchEmbedding(nn.Module):\n",
    "    def __init__(self, config: SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patch_layer = nn.Conv2d(in_channels=3,\n",
    "                                     out_channels=config.hidden_size,\n",
    "                                     kernel_size=config.patch_size,\n",
    "                                     stride=config.patch_size,\n",
    "                                     padding='valid')\n",
    "        \n",
    "        # total number of pixel in image = 224*224, total pixel in a patch is 16*16 , so total patch number = 224*224 / 16*16\n",
    "        self.patch_num = config.image_size**2 // config.patch_size**2\n",
    "        \n",
    "        self.pos_embedding = nn.Embedding(num_embeddings=self.patch_num,\n",
    "                                          embedding_dim=config.hidden_size)\n",
    "        \n",
    "        self.register_buffer('position_ids',\n",
    "                             torch.arange(0, self.patch_num).view(1, -1),\n",
    "                             persistent=False) # `persistent` = do we need it as a part of module state dict\n",
    "        \n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.patch_layer(x) # (Batch , channel, height, width) => (Batch, hidden_size, height, width)\n",
    "        # x = torch.flatten(x, start_dim=-2, end_dim=-1) => [Batch, hidden_size, heigt*widht = num_patch]\n",
    "        x = x.view(x.shape[0], x.shape[1], x.shape[2] * x.shape[3]) \n",
    "        # (Batch, num_patch, hidden_size)\n",
    "        x = x.transpose(-2, -1)\n",
    "        x = x + self.pos_embedding(self.position_ids)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a52ce6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.eps = config.layer_norm_eps\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.scale = nn.Parameter(torch.ones(self.hidden_size))\n",
    "        self.shift = nn.Parameter(torch.zeros(self.hidden_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_mean = torch.mean(x, dim=-1, keepdim=True)\n",
    "        x_std = torch.std(x, dim=-1, keepdim=True)\n",
    "        x_norm = (x - x_mean) / (x_std + self.eps)\n",
    "        return x_norm * self.scale + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2f5112b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiglipAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.hidden_size = config.hidden_size\n",
    "        assert self.hidden_size % self.num_attention_heads == 0, \"hidden size should be divisible by num_attention_heads\"\n",
    "        self.attn_head_size = self.hidden_size // self.num_attention_heads\n",
    "        \n",
    "        self.q_W = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.k_W = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.v_W = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.out_W = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, patch_len, _ = x.shape\n",
    "        \n",
    "        queries = self.q_W(x)\n",
    "        keys = self.k_W(x)\n",
    "        values = self.v_W(x)\n",
    "        \n",
    "        # (Batch, patch, hidden_size) -> (Batch, patch, heads, head_embed) -> (batch, heads, patch, head_embed)\n",
    "        queries = queries.view(batch_size, patch_len, self.num_attention_heads, self.attn_head_size).transpose(1, 2)\n",
    "        keys = keys.view(batch_size, patch_len, self.num_attention_heads, self.attn_head_size).transpose(1, 2)\n",
    "        values = values.view(batch_size, patch_len, self.num_attention_heads, self.attn_head_size).transpose(1, 2)\n",
    "        \n",
    "        # attention_logits (b, head , patch, patch)\n",
    "        attention_logits = (queries @ keys.transpose(-1, -2)) * self.attn_head_size**-0.5\n",
    "        attention_scores = nn.functional.softmax(attention_logits, dim=-1, dtype=torch.float32)\n",
    "        \n",
    "        # (b, head, patch, patch) * (b, head, patch, embed) => (b, head, patch, embed)\n",
    "        contextual_embeddings = attention_scores @ values\n",
    "        \n",
    "        # (b, head, patch, embed) -> (b, patch, head, embed)\n",
    "        contextual_embeddings = contextual_embeddings.transpose(1, 2)\n",
    "        \n",
    "        contextual_embeddings = contextual_embeddings.contiguous().view(batch_size, patch_len, self.num_attention_heads*self.attn_head_size)\n",
    "        \n",
    "        contextual_embeddings = self.out_W(contextual_embeddings)\n",
    "        \n",
    "        return contextual_embeddings, attention_scores \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97cb9d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiglipEncoderLayer(nn.Module):\n",
    "    def __init__(self, config: SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.layer_norm_1 = LayerNorm(config)\n",
    "        self.attention = SiglipAttention(config)\n",
    "        self.layer_norm_2 = LayerNorm(config)\n",
    "        self.final_linear = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.intermediate_size),\n",
    "            nn.GELU('tanh'),\n",
    "            nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        )        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual =  x\n",
    "        x = self.layer_norm_1(x)\n",
    "        x, _ = self.attention(x)\n",
    "        x = x + residual\n",
    "        \n",
    "        residual = x\n",
    "        x = self.layer_norm_2(x)\n",
    "        x = self.final_linear(x)\n",
    "        x = x + residual\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d19b8a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiglipEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_layer = config.num_hidden_layers\n",
    "        self.encoder_layers = nn.ModuleList(\n",
    "            [SiglipEncoderLayer(config) for _ in range(self.n_layer)]\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d115ee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiglipVisionTransformer(nn.Module):\n",
    "    def __init__(self, config: SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.patch_embedding = SiglipPatchEmbedding(config)\n",
    "        self.encoder = SiglipEncoder(config) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c1dd5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiglipVisionModel(nn.Module):\n",
    "    def __init__(self, config: SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.visual_model = SiglipVisionTransformer(config)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # [Batch, channel, height, width] -> [Batch, patch, embed_dim]\n",
    "        return self.visual_model(x)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6071a767",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_STANDARD_MEAN = [0.5, 0.5, 0.5]\n",
    "IMAGENET_STANDARD_STD = [0.5, 0.5, 0.5]\n",
    "\n",
    "def resize(image: Image,\n",
    "           size: Tuple[int, int],\n",
    "           resample: Image.Resampling=None,\n",
    "           reducing_gap: Optional[int]=None):\n",
    "    height, width = size\n",
    "    resize_image = image.resize(\n",
    "        (width, height), resample=resample, reducing_gap=reducing_gap\n",
    "    )\n",
    "    \n",
    "    return resize_image\n",
    "\n",
    "def rescale(image: np.ndarray,\n",
    "            scale: float,\n",
    "            dtype: np.dtype = np.float32):\n",
    "    rescaled_image = image * scale\n",
    "    rescaled_image = rescaled_image.astype(dtype)\n",
    "    return rescaled_image\n",
    "\n",
    "def normalize(image: Image.Image,\n",
    "              mean: Union[float, Iterable[float]],\n",
    "              std: Union[float, Iterable[float]],\n",
    "              ):\n",
    "    mean = np.array(mean)\n",
    "    std = np.array(std)\n",
    "    normalized_image = (image - mean) / std\n",
    "    \n",
    "    return normalized_image\n",
    "    \n",
    "\n",
    "def process_image(\n",
    "    images: List[Image.Image],\n",
    "    size: Dict[str, int] = None,\n",
    "    resample: Image.Resampling = None,\n",
    "    rescale_factor: float=None,\n",
    "    image_mean: Optional[Union[float, List[float]]] = None,\n",
    "    image_std: Optional[Union[float, List[float]]] = None,\n",
    "):\n",
    "    height, width = size[0], size[1]\n",
    "    \n",
    "    images = [\n",
    "        resize(image=image, size=(height, width), resample=resample) for image in images\n",
    "    ]\n",
    "    \n",
    "    images = [np.array(image) for image in images]\n",
    "    \n",
    "    images = [rescale(image, scale=rescale_factor) for image in images]\n",
    "    \n",
    "    images = [normalize(image, mean=image_mean, std=image_std) for image in images]\n",
    "    \n",
    "    images = [image.transpose(2, 0, 1) for image in images]\n",
    "    \n",
    "    return images\n",
    "    \n",
    "\n",
    "class PaliGemmaProcessor:\n",
    "    \n",
    "    IMAGE_TOKEN = \"<image>\"\n",
    "    \n",
    "    def __init__(self, tokenizer, num_image_token: int, image_size: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.image_seq_len = num_image_token # num_patches\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        # Special token to add\n",
    "        token_to_add = {'additional_special_token': [self.IMAGE_TOKEN]}\n",
    "        tokenizer.add_special_tokens(token_to_add) # added special token in hugging face tokenizer\n",
    "        \n",
    "        EXTRA_TOKENS = [\n",
    "            f\"<loc{i:04d}>\" for i in range(1024)\n",
    "        ]\n",
    "        \n",
    "        EXTRA_TOKENS += [\n",
    "            f'<seg{i:03d}>' for i in range(128)\n",
    "        ]\n",
    "        \n",
    "        tokenizer.add_tokens(EXTRA_TOKENS)\n",
    "        \n",
    "        self.image_token_id = tokenizer.convert_tokens_to_ids(self.IMAGE_TOKEN)\n",
    "        \n",
    "        tokenizer.add_bos_token = False\n",
    "        tokenizer.add_eos_token = False\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __call__(self, \n",
    "                 texts: List[str],\n",
    "                 images: List[Image.Image],\n",
    "                 padding=False,\n",
    "                 truncate=False) -> dict:\n",
    "        \n",
    "        assert len(images) == len(texts), \"the number of text and image should be same\"\n",
    "        \n",
    "        pixel_values = process_image(\n",
    "            images,\n",
    "            size=(self.image_size, self.image_size),\n",
    "            resample=Image.Resampling.BICUBIC,\n",
    "            rescale_factor=1 / 255.0,\n",
    "            image_mean=IMAGENET_STANDARD_MEAN,\n",
    "            image_std=IMAGENET_STANDARD_STD\n",
    "        )\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb934bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
