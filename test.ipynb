{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00f01490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple, List, Dict, Iterable, Union\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2d0edb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check device\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78006ff",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "297d02b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiglipVisionConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size=768,\n",
    "        intermediate_size=3072,\n",
    "        num_hidden_layers=12,\n",
    "        num_attention_heads=12,\n",
    "        num_channels=3,\n",
    "        image_size=224,\n",
    "        patch_size=16,\n",
    "        layer_norm_eps=1e-6,\n",
    "        attention_dropout=0.0,\n",
    "        num_image_tokens:int = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.num_channels = num_channels\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.num_image_tokens = num_image_tokens\n",
    "        \n",
    "config = SiglipVisionConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc448f0",
   "metadata": {},
   "source": [
    "## Visual Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d115ee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer taks a raw image -> patch_tokens with positional embeddings\n",
    "class SiglipPatchEmbedding(nn.Module):\n",
    "    def __init__(self, config: SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patch_layer = nn.Conv2d(in_channels=3,\n",
    "                                     out_channels=config.hidden_size,\n",
    "                                     kernel_size=config.patch_size,\n",
    "                                     stride=config.patch_size,\n",
    "                                     padding='valid')\n",
    "        \n",
    "        # total number of pixel in image = 224*224, total pixel in a patch is 16*16 , so total patch number = 224*224 / 16*16\n",
    "        self.patch_num = config.image_size**2 // config.patch_size**2\n",
    "        \n",
    "        self.pos_embedding = nn.Embedding(num_embeddings=self.patch_num,\n",
    "                                          embedding_dim=config.hidden_size)\n",
    "        \n",
    "        self.register_buffer('position_ids',\n",
    "                             torch.arange(0, self.patch_num).view(1, -1),\n",
    "                             persistent=False) # `persistent` = do we need it as a part of module state dict\n",
    "        \n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.patch_layer(x) # (Batch , channel, height, width) => (Batch, hidden_size, height, width)\n",
    "        # x = torch.flatten(x, start_dim=-2, end_dim=-1) => [Batch, hidden_size, heigt*widht = num_patch]\n",
    "        x = x.view(x.shape[0], x.shape[1], x.shape[2] * x.shape[3]) \n",
    "        # (Batch, num_patch, hidden_size)\n",
    "        x = x.transpose(-2, -1)\n",
    "        x = x + self.pos_embedding(self.position_ids)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.eps = config.layer_norm_eps\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.scale = nn.Parameter(torch.ones(self.hidden_size))\n",
    "        self.shift = nn.Parameter(torch.zeros(self.hidden_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_mean = torch.mean(x, dim=-1, keepdim=True)\n",
    "        x_std = torch.std(x, dim=-1, keepdim=True)\n",
    "        x_norm = (x - x_mean) / (x_std + self.eps)\n",
    "        return x_norm * self.scale + self.shift\n",
    "\n",
    "class SiglipAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.hidden_size = config.hidden_size\n",
    "        assert self.hidden_size % self.num_attention_heads == 0, \"hidden size should be divisible by num_attention_heads\"\n",
    "        self.attn_head_size = self.hidden_size // self.num_attention_heads\n",
    "        \n",
    "        self.q_W = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.k_W = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.v_W = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.out_W = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, patch_len, _ = x.shape\n",
    "        \n",
    "        queries = self.q_W(x)\n",
    "        keys = self.k_W(x)\n",
    "        values = self.v_W(x)\n",
    "        \n",
    "        # (Batch, patch, hidden_size) -> (Batch, patch, heads, head_embed) -> (batch, heads, patch, head_embed)\n",
    "        queries = queries.view(batch_size, patch_len, self.num_attention_heads, self.attn_head_size).transpose(1, 2)\n",
    "        keys = keys.view(batch_size, patch_len, self.num_attention_heads, self.attn_head_size).transpose(1, 2)\n",
    "        values = values.view(batch_size, patch_len, self.num_attention_heads, self.attn_head_size).transpose(1, 2)\n",
    "        \n",
    "        # attention_logits (b, head , patch, patch)\n",
    "        attention_logits = (queries @ keys.transpose(-1, -2)) * self.attn_head_size**-0.5\n",
    "        attention_scores = nn.functional.softmax(attention_logits, dim=-1, dtype=torch.float32)\n",
    "        \n",
    "        # (b, head, patch, patch) * (b, head, patch, embed) => (b, head, patch, embed)\n",
    "        contextual_embeddings = attention_scores @ values\n",
    "        \n",
    "        # (b, head, patch, embed) -> (b, patch, head, embed)\n",
    "        contextual_embeddings = contextual_embeddings.transpose(1, 2)\n",
    "        \n",
    "        contextual_embeddings = contextual_embeddings.contiguous().view(batch_size, patch_len, self.num_attention_heads*self.attn_head_size)\n",
    "        \n",
    "        contextual_embeddings = self.out_W(contextual_embeddings)\n",
    "        \n",
    "        return contextual_embeddings, attention_scores \n",
    "        \n",
    "\n",
    "\n",
    "class SiglipEncoderLayer(nn.Module):\n",
    "    def __init__(self, config: SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.layer_norm_1 = LayerNorm(config)\n",
    "        self.attention = SiglipAttention(config)\n",
    "        self.layer_norm_2 = LayerNorm(config)\n",
    "        self.final_linear = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.intermediate_size),\n",
    "            nn.GELU('tanh'),\n",
    "            nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        )        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual =  x\n",
    "        x = self.layer_norm_1(x)\n",
    "        x, _ = self.attention(x)\n",
    "        x = x + residual\n",
    "        \n",
    "        residual = x\n",
    "        x = self.layer_norm_2(x)\n",
    "        x = self.final_linear(x)\n",
    "        x = x + residual\n",
    "        \n",
    "        return x\n",
    "\n",
    "class SiglipEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_layer = config.num_hidden_layers\n",
    "        self.encoder_layers = nn.ModuleList(\n",
    "            [SiglipEncoderLayer(config) for _ in range(self.n_layer)]\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "class SiglipVisionTransformer(nn.Module):\n",
    "    def __init__(self, config: SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.patch_embedding = SiglipPatchEmbedding(config)\n",
    "        self.encoder = SiglipEncoder(config) \n",
    "        self.post_layer_norm = LayerNorm(config)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # (b, channel, height, width) -> (b, patch_len, patch_embedding)\n",
    "        x = self.patch_embedding(x)\n",
    "        # (b, num_patch, embedding_size) same as before\n",
    "        x = self.encoder(x) \n",
    "        x = self.post_layer_norm(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "        \n",
    "class SiglipVisionModel(nn.Module):\n",
    "    def __init__(self, config: SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.visual_model = SiglipVisionTransformer(config)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # [Batch, channel, height, width] -> [Batch, patch, embed_dim]\n",
    "        return self.visual_model(x)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d197a7c8",
   "metadata": {},
   "source": [
    "## Siglip Input Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6071a767",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_STANDARD_MEAN = [0.5, 0.5, 0.5]\n",
    "IMAGENET_STANDARD_STD = [0.5, 0.5, 0.5]\n",
    "\n",
    "def resize(image: Image,\n",
    "           size: Tuple[int, int],\n",
    "           resample: Image.Resampling=None,\n",
    "           reducing_gap: Optional[int]=None):\n",
    "    height, width = size\n",
    "    resize_image = image.resize(\n",
    "        (width, height), resample=resample, reducing_gap=reducing_gap\n",
    "    )\n",
    "    \n",
    "    return resize_image\n",
    "\n",
    "def rescale(image: np.ndarray,\n",
    "            scale: float,\n",
    "            dtype: np.dtype = np.float32):\n",
    "    rescaled_image = image * scale\n",
    "    rescaled_image = rescaled_image.astype(dtype)\n",
    "    return rescaled_image\n",
    "\n",
    "def normalize(image: Image.Image,\n",
    "              mean: Union[float, Iterable[float]],\n",
    "              std: Union[float, Iterable[float]],\n",
    "              ):\n",
    "    mean = np.array(mean)\n",
    "    std = np.array(std)\n",
    "    normalized_image = (image - mean) / std\n",
    "    \n",
    "    return normalized_image\n",
    "    \n",
    "\n",
    "def process_image(\n",
    "    images: List[Image.Image],\n",
    "    size: Dict[str, int] = None,\n",
    "    resample: Image.Resampling = None,\n",
    "    rescale_factor: float=None,\n",
    "    image_mean: Optional[Union[float, List[float]]] = None,\n",
    "    image_std: Optional[Union[float, List[float]]] = None,\n",
    "):\n",
    "    height, width = size[0], size[1]\n",
    "    \n",
    "    images = [\n",
    "        resize(image=image, size=(height, width), resample=resample) for image in images\n",
    "    ]\n",
    "    \n",
    "    images = [np.array(image) for image in images]\n",
    "    \n",
    "    images = [rescale(image, scale=rescale_factor) for image in images]\n",
    "    \n",
    "    images = [normalize(image, mean=image_mean, std=image_std) for image in images]\n",
    "    \n",
    "    images = [image.transpose(2, 0, 1) for image in images] # image format -> torch image formate\n",
    "    \n",
    "    return images\n",
    "\n",
    "def add_image_tokens_to_prompt(prefix_prompt,\n",
    "                               bos_token,\n",
    "                               image_seq_len,\n",
    "                               image_token):\n",
    "    \n",
    "    return f\"{image_token * image_seq_len}{bos_token}{prefix_prompt}\\n\"\n",
    "    \n",
    "    \n",
    "\n",
    "class PaliGemmaProcessor:\n",
    "    \n",
    "    IMAGE_TOKEN = \"<image>\"\n",
    "    \n",
    "    def __init__(self, tokenizer, num_image_token: int, image_size: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.image_seq_len = num_image_token # num_patches\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        # Special token to add\n",
    "        token_to_add = {'additional_special_token': [self.IMAGE_TOKEN]}\n",
    "        tokenizer.add_special_tokens(token_to_add) # added special token in hugging face tokenizer\n",
    "        \n",
    "        EXTRA_TOKENS = [\n",
    "            f\"<loc{i:04d}>\" for i in range(1024)\n",
    "        ]\n",
    "        \n",
    "        EXTRA_TOKENS += [\n",
    "            f'<seg{i:03d}>' for i in range(128)\n",
    "        ]\n",
    "        \n",
    "        tokenizer.add_tokens(EXTRA_TOKENS)\n",
    "        \n",
    "        self.image_token_id = tokenizer.convert_tokens_to_ids(self.IMAGE_TOKEN)\n",
    "        \n",
    "        tokenizer.add_bos_token = False\n",
    "        tokenizer.add_eos_token = False\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __call__(self, \n",
    "                 texts: List[str],\n",
    "                 images: List[Image.Image],\n",
    "                 padding=False,\n",
    "                 truncate=False) -> dict:\n",
    "        \n",
    "        assert len(images) == len(texts), \"the number of text and image should be same\"\n",
    "        \n",
    "        pixel_values = process_image(\n",
    "            images,\n",
    "            size=(self.image_size, self.image_size),\n",
    "            resample=Image.Resampling.BICUBIC,\n",
    "            rescale_factor=1 / 255.0,\n",
    "            image_mean=IMAGENET_STANDARD_MEAN,\n",
    "            image_std=IMAGENET_STANDARD_STD\n",
    "        )\n",
    "        \n",
    "        pixel_values = np.stack(pixel_values, axis=0)\n",
    "        pixel_values = torch.tensor(pixel_values)\n",
    "        \n",
    "        input_string = [\n",
    "            add_image_tokens_to_prompt(\n",
    "                prefix_prompt=prompt,\n",
    "                bos_token=self.tokenizer.bos_token,\n",
    "                image_seq_len=self.image_seq_len,\n",
    "                image_token=self.IMAGE_TOKEN\n",
    "            )\n",
    "            for prompt in texts\n",
    "        ]\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            input_string,\n",
    "            return_tesors='pt',\n",
    "            padding=padding,\n",
    "            truncation=truncate,\n",
    "        )\n",
    "        \n",
    "        return {'pixel_values': pixel_values, **inputs} # input_ids and attention mask\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb934bb",
   "metadata": {},
   "source": [
    "## Langauge Model - Gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1f4c8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaConfig:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 hidden_size,\n",
    "                 intermediate_size,\n",
    "                 num_hidden_layers,\n",
    "                 num_attention_heads,\n",
    "                 num_key_value_heads,\n",
    "                 head_dim=256,\n",
    "                 max_position_embeddings=2048,\n",
    "                 rms_norm_eps=1e-6,\n",
    "                 rope_theta=100000.0,\n",
    "                 attention_bias=False,\n",
    "                 attention_dropout=0.0,\n",
    "                 pad_token_id=None,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.num_key_value_heads = num_key_value_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.rms_norm_eps = rms_norm_eps\n",
    "        self.rope_theta = rope_theta\n",
    "        self.attention_bias = attention_bias\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.pad_token_id = pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbe4da0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KVCache():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.key_cache = List[torch.Tensor] = []\n",
    "        self.value_cache = List[torch.Tensor] = []\n",
    "        \n",
    "    def num_items(self) -> int:\n",
    "        if len(self.key_cache) == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            # [batch, heads, seq_len, head_dim]\n",
    "            return self.key_cache[0].shape[-2] # seq len\n",
    "        \n",
    "    def update(self, key_state, value_state, layer_idx):\n",
    "        if len(self.key_cache) <= layer_idx: # kv_cache is a list for each layer, so the len tells how much layer kv cache we stored\n",
    "            self.key_cache(key_state)\n",
    "            self.value_cache(value_state)\n",
    "        \n",
    "        else:\n",
    "            self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_state], dim=-2)\n",
    "            self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_state], dim=-2)\n",
    "        \n",
    "        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8c8a891",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PaligemmaConfig():\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vision_config=None,\n",
    "        text_config=None,\n",
    "        ignore_index=-100,\n",
    "        image_token_index=256000,\n",
    "        vocab_size=257152,\n",
    "        projection_dim=2048,\n",
    "        hidden_size=2048,\n",
    "        pad_token_id = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.vision_config = SiglipVisionConfig(**vision_config)\n",
    "        self.text_config = GemmaConfig(**text_config)\n",
    "        self.is_encoder_decoder = False\n",
    "        self.ignore_index = ignore_index\n",
    "        self.image_token_index = image_token_index\n",
    "        self.projection_dim = projection_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.pad_token_id = pad_token_id \n",
    "        self.vocab_size = self.text_config.vocab_size\n",
    "        \n",
    "        self.text_config.num_image_tokens = self.vision_config.image_size**2 // self.vision_config.patch_size**2\n",
    "        self.vision_config.projection_dim = self.projection_dim\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5261b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaliGemmaMultiModelProjector(nn.Module):\n",
    "    def __init__(self, config: PaligemmaConfig):\n",
    "        super().__init__()\n",
    "        self.linear_proj = nn.Linear(config.vision_config.hidden_size, config.vision_config.projection_dim, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear_proj(x) # convert each image embedding to be same size as text embedding for concatination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b302afad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaRMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weights = nn.Parameter(torch.zeros(hidden_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keep_dim=True) + self.eps)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70917558",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaRotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim # it is set to the head_dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.base = base\n",
    "\n",
    "        # Calculate the theta according to the formula theta_i = base^(-2i/dim) where i = 0, 1, 2, ..., dim // 2\n",
    "        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float() / self.dim))\n",
    "        self.register_buffer(\"inv_freq\", tensor=inv_freq, persistent=False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x, position_ids, seq_len=None):\n",
    "        # x: [bs, num_attention_heads, seq_len, head_size]\n",
    "        self.inv_freq.to(x.device)\n",
    "        # Copy the inv_freq tensor for batch in the sequence\n",
    "        # inv_freq_expanded: [Batch_Size, Head_Dim // 2, 1]\n",
    "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n",
    "        # position_ids_expanded: [Batch_Size, 1, Seq_Len]\n",
    "        position_ids_expanded = position_ids[:, None, :].float()\n",
    "        device_type = x.device.type\n",
    "        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n",
    "        with torch.autocast(device_type=device_type, enabled=False):\n",
    "            # Multiply each theta by the position (which is the argument of the sin and cos functions)\n",
    "            # freqs: [Batch_Size, Head_Dim // 2, 1] @ [Batch_Size, 1, Seq_Len] --> [Batch_Size, Seq_Len, Head_Dim // 2]\n",
    "            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
    "            # emb: [Batch_Size, Seq_Len, Head_Dim]\n",
    "            emb = torch.cat((freqs, freqs), dim=-1)\n",
    "            # cos, sin: [Batch_Size, Seq_Len, Head_Dim]\n",
    "            cos = emb.cos()\n",
    "            sin = emb.sin()\n",
    "        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n",
    "\n",
    "\n",
    "def rotate_half(x):\n",
    "    # Build the [-x2, x1, -x4, x3, ...] tensor for the sin part of the positional encoding.\n",
    "    x1 = x[..., : x.shape[-1] // 2] # Takes the first half of the last dimension\n",
    "    x2 = x[..., x.shape[-1] // 2 :] # Takes the second half of the last dimension\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, unsqueeze_dim=1):\n",
    "    cos = cos.unsqueeze(unsqueeze_dim) # Add the head dimension\n",
    "    sin = sin.unsqueeze(unsqueeze_dim) # Add the head dimension\n",
    "    # Apply the formula (34) of the Rotary Positional Encoding paper.\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7225c2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaMLP(nn.Module):\n",
    "    def __init__(self, config: GemmaConfig):\n",
    "        super().__init__()\n",
    "        self.up_proj = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        up_proj = self.up_proj(x)\n",
    "        gate_proj = nn.functional.gelu(gate_proj(x), approximate='tanh')\n",
    "        gated_up_proj = up_proj * gate_proj\n",
    "        \n",
    "        return self.down_proj(gated_up_proj)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1db2ccfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaAttention(nn.Module):\n",
    "    def __init__(self, config: GemmaConfig, layer_idx: Optional[int] = None):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "\n",
    "        self.attention_dropout = config.attention_dropout\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = config.head_dim\n",
    "        self.num_key_value_heads = config.num_key_value_heads\n",
    "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
    "        self.max_position_embeddings = config.max_position_embeddings\n",
    "        self.rope_theta = config.rope_theta\n",
    "        self.is_causal = True\n",
    "\n",
    "        assert self.hidden_size % self.num_heads == 0            \n",
    "\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
    "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n",
    "        \n",
    "        self.rotary_emb = GemmaRotaryEmbedding(\n",
    "            self.head_dim,\n",
    "            max_position_embeddings=self.max_position_embeddings,\n",
    "            base=self.rope_theta,\n",
    "        )\n",
    "        \n",
    "    \n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        kv_cache: Optional[KVCache] = None,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "            bsz, q_len, _ = hidden_states.size() # [Batch_Size, Seq_Len, Hidden_Size]\n",
    "            # [Batch_Size, Seq_Len, Num_Heads_Q * Head_Dim]\n",
    "            query_states = self.q_proj(hidden_states)\n",
    "            # [Batch_Size, Seq_Len, Num_Heads_KV * Head_Dim]\n",
    "            key_states = self.k_proj(hidden_states)\n",
    "            # [Batch_Size, Seq_Len, Num_Heads_KV * Head_Dim]\n",
    "            value_states = self.v_proj(hidden_states)\n",
    "            # [Batch_Size, Num_Heads_Q, Seq_Len, Head_Dim]\n",
    "            query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "            # [Batch_Size, Num_Heads_KV, Seq_Len, Head_Dim]\n",
    "            key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "            # [Batch_Size, Num_Heads_KV, Seq_Len, Head_Dim]\n",
    "            value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "            # [Batch_Size, Seq_Len, Head_Dim], [Batch_Size, Seq_Len, Head_Dim]\n",
    "            cos, sin = self.rotary_emb(value_states, position_ids, seq_len=None)\n",
    "            # [Batch_Size, Num_Heads_Q, Seq_Len, Head_Dim], [Batch_Size, Num_Heads_KV, Seq_Len, Head_Dim]\n",
    "            query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "            if kv_cache is not None:\n",
    "                key_states, value_states = kv_cache.update(key_states, value_states, self.layer_idx)\n",
    "\n",
    "            # Repeat the key and values to match the number of heads of the query\n",
    "            key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
    "            value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
    "            # Perform the calculation as usual, Q * K^T / sqrt(head_dim). Shape: [Batch_Size, Num_Heads_Q, Seq_Len_Q, Seq_Len_KV]\n",
    "            attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "\n",
    "            assert attention_mask is not None\n",
    "            attn_weights = attn_weights + attention_mask\n",
    "\n",
    "            # Apply the softmax\n",
    "            # [Batch_Size, Num_Heads_Q, Seq_Len_Q, Seq_Len_KV]\n",
    "            attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "            # Apply the dropout\n",
    "            attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n",
    "            # Multiply by the values. [Batch_Size, Num_Heads_Q, Seq_Len_Q, Seq_Len_KV] x [Batch_Size, Num_Heads_KV, Seq_Len_KV, Head_Dim] -> [Batch_Size, Num_Heads_Q, Seq_Len_Q, Head_Dim]\n",
    "            attn_output = torch.matmul(attn_weights, value_states)\n",
    "\n",
    "            if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n",
    "                raise ValueError(\n",
    "                    f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n",
    "                    f\" {attn_output.size()}\"\n",
    "                )\n",
    "            # Make sure the sequence length is the second dimension. # [Batch_Size, Num_Heads_Q, Seq_Len_Q, Head_Dim] -> [Batch_Size, Seq_Len_Q, Num_Heads_Q, Head_Dim]\n",
    "            attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "            # Concatenate all the heads together. [Batch_Size, Seq_Len_Q, Num_Heads_Q, Head_Dim] -> [Batch_Size, Seq_Len_Q, Num_Heads_Q * Head_Dim]\n",
    "            attn_output = attn_output.view(bsz, q_len, -1)\n",
    "            # Multiply by W_o. [Batch_Size, Seq_Len_Q, Hidden_Size]\n",
    "            attn_output = self.o_proj(attn_output)\n",
    "\n",
    "            return attn_output, attn_weights\n",
    "\n",
    "\n",
    "def repeat_kv(hidden_state: torch.Tensor, n_rep: int): \n",
    "    batch_size, num_key_value_heads, seq_len, head_dim = hidden_state.shape\n",
    "    \n",
    "    if n_rep==1:\n",
    "        return hidden_state\n",
    "    \n",
    "    hidden_state = hidden_state[:, :, None, :, :].expand(batch_size, num_key_value_heads, n_rep, seq_len, head_dim)\n",
    "    return hidden_state.reshape(batch_size, num_key_value_heads * n_rep, seq_len, head_dim)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab15d701",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config: GemmaConfig, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.attn_lyr = GemmaAttention(config=config, layer_idx=layer_idx)\n",
    "        self.mlp = GemmaMLP(config)\n",
    "        self.pre_layernorm = GemmaRMSNorm(self.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.post_layernorm = GemmaRMSNorm(self.hidden_size, eps=config.rms_norm_eps)    \n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_state: torch.Tensor = None,\n",
    "        attention_mask: torch.Tensor = None,\n",
    "        positional_ids: Optional[torch.Tensor] = None,\n",
    "        kv_cache: Optional[KVCache] = None,\n",
    "    ):\n",
    "        residual = hidden_state\n",
    "        hidden_state = self.pre_layernorm(hidden_state)\n",
    "        \n",
    "        hidden_state, _ = self.attn_lyr(\n",
    "            hidden_state=hidden_state,\n",
    "            attention_mask=attention_mask,\n",
    "            positional_ids=positional_ids,\n",
    "            kv_cache=kv_cache\n",
    "        )\n",
    "        \n",
    "        hidden_state = residual + hidden_state\n",
    "        \n",
    "        residual = hidden_state\n",
    "        hidden_state = self.post_layernorm(hidden_state)\n",
    "        hidden_state = self.mlp(hidden_state)\n",
    "        hidden_state = hidden_state + residual\n",
    "        \n",
    "        return hidden_state\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8da67b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaModel(nn.Module):\n",
    "    def __init__(self, config: GemmaConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.vocab_size = config.vocab_size\n",
    "        \n",
    "        self.embed_token = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            GemmaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)\n",
    "        ])\n",
    "        \n",
    "        self.norm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        \n",
    "    def get_input_embedding(self):\n",
    "        return self.embed_token\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        attention_mask,\n",
    "        positional_ids,\n",
    "        input_embeds,\n",
    "        kv_cache: Optional[KVCache] = None\n",
    "    ):\n",
    "        hidden_state = input_embeds\n",
    "        normalizer = torch.tensor(self.config.hidden_size**0.5, dtype=hidden_state.dtype)\n",
    "        hidden_state = hidden_state * normalizer\n",
    "        \n",
    "        for decoder in self.layers:\n",
    "            hidden_state = decoder(\n",
    "                hidden_state,\n",
    "                attention_mask=attention_mask,\n",
    "                positional_ids=positional_ids,\n",
    "                kv_cache=kv_cache\n",
    "            )\n",
    "        \n",
    "        hidden_state = self.norm(hidden_state)\n",
    "        \n",
    "        return hidden_state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58821955",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaCasualLM(nn.Module):\n",
    "    def __init__(self, config: GemmaConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.model = GemmaModel(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.head_dim, config.vocab_size, bias=False)\n",
    "    \n",
    "    def get_input_embedding(self):\n",
    "        return self.model.embed_tokens\n",
    "    \n",
    "    def tie_weights(self):\n",
    "        self.lm_head.weight = self.model.embed_tokens.weight\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        positional_ids: Optional[torch.LongTensor] = None,\n",
    "        input_embeds: Optional[torch.FloatTensor] = None,\n",
    "        kv_cache: Optional[KVCache] = None\n",
    "    ) -> Tuple:\n",
    "        \n",
    "        output = self.model(\n",
    "            attention_mask=attention_mask,\n",
    "            positional_ids=positional_ids,\n",
    "            input_embeds=input_embeds,\n",
    "            kv_cache=kv_cache\n",
    "        )\n",
    "        hidden_state = output\n",
    "        logits = self.lm_head(hidden_state)\n",
    "        logits = logits.float()\n",
    "        \n",
    "        return_data =  {\n",
    "            \"logits\" : logits\n",
    "        }\n",
    "        \n",
    "        if kv_cache is not None:\n",
    "            return_data['kv_cache'] = kv_cache\n",
    "            \n",
    "        return return_data\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee54bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d733e14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:12: SyntaxWarning: \"is not\" with 'int' literal. Did you mean \"!=\"?\n",
      "<>:12: SyntaxWarning: \"is not\" with 'int' literal. Did you mean \"!=\"?\n",
      "/var/folders/81/8b_lw5jj52qfv4kmf14tk0p00000gn/T/ipykernel_84421/2638568866.py:12: SyntaxWarning: \"is not\" with 'int' literal. Did you mean \"!=\"?\n",
      "  self.pad_token = self.config.pad_token_id if self.confg.pad_token_id is not -1 else -1\n"
     ]
    }
   ],
   "source": [
    "class PaliGemmaForConditionalGeneration(nn.Module):\n",
    "    def __init__(self, config: PaligemmaConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        self.vision_model = SiglipVisionModel(config.vision_config)\n",
    "        self.multi_model_projector = PaliGemmaMultiModelProjector(config) # A Linear projector that standardize text and image embedding size\n",
    "        self.vocab_size = config.vacab_size\n",
    "        \n",
    "        self.language_model = GemmaCasualLM(config.text_config)\n",
    "        \n",
    "        self.pad_token = self.config.pad_token_id if self.confg.pad_token_id is not -1 else -1\n",
    "        \n",
    "    # share the weights of embedding (vocab_size * embdding_size) to output projecting layer weights (embedding_size * vocab_size)\n",
    "    def tie_weights(self):\n",
    "        return self.language_model.tie_weights()\n",
    "    \n",
    "    def _merge_input_ids_with_image_features(\n",
    "        self, image_features: torch.Tensor,\n",
    "        input_embeds: torch.Tensor,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        kv_cache\n",
    "    ):\n",
    "        embed_dim = image_features.shape[-1]\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        dtype, device = input_embeds.dtype, input_embeds.device\n",
    "        \n",
    "        # same scaling as in the attention for same reason, for consistent magnitude event the number parameter increase (different size of models)\n",
    "        scaled_image_features = image_features / (self.config.hidden_size**0.5)\n",
    "        final_embedding = torch.zeros(batch_size, seq_len, embed_dim, dtype=dtype, device=device)\n",
    "        \n",
    "        text_mask = (input_ids != self.config.image_token_index) & (input_ids != self.pad_token_id)\n",
    "        image_mask = (input_ids == self.config.image_token_index)\n",
    "        pad_mask = (input_ids == self.pad_token_id)\n",
    "        \n",
    "        text_mask_expanded = text_mask.unsqueeze(-1).expand(-1, -1, embed_dim)\n",
    "        image_mask_expanded = text_mask.unsqueeze(-1).expand(-1, -1, embed_dim)\n",
    "        pad_mask_expanded = text_mask.unsqueeze(-1).expand(-1, -1, embed_dim)\n",
    "        \n",
    "        # final_embedding (b, total_seq_len, embed_size)\n",
    "        # all mask = (b, total_seq_len, embed_size )\n",
    "        # image_feature (b, patch_len, embed)\n",
    "        final_embedding = torch.where(text_mask_expanded, input_embeds, final_embedding)\n",
    "        # we use masked_scatter since our scale_image_feature is different shape than final multimodel input shape, so torch.where gives error, but masked_scatter do the same thing in different way, you can learn it \n",
    "        final_embedding = torch.masked_scatter(final_embedding, image_mask_expanded, scaled_image_features)\n",
    "        final_embedding = torch.where(pad_mask_expanded, torch.zeros_like(final_embedding), final_embedding)\n",
    "        \n",
    "        # KV CACHE\n",
    "        min_dtype = torch.finfo(dtype=dtype)\n",
    "        q_len = input_embeds.shape[1]\n",
    "        \n",
    "        if kv_cache is None or kv_cache.num_items() == 1:\n",
    "            # Prefilling phase\n",
    "            casual_mask =  torch.full(size=(batch_size, q_len, q_len),\n",
    "                                      fill_value=0,\n",
    "                                      dtype=dtype,\n",
    "                                      device=device)\n",
    "        else:\n",
    "            \n",
    "            assert q_len == 1, \"when enable kv cache the query len should be one\"\n",
    "            \n",
    "            kv_len = kv_cache.num_items() + q_len\n",
    "            \n",
    "            casual_mask = torch.full(size=(batch_size, q_len, kv_len),\n",
    "                                      fill_value=0,\n",
    "                                      dtype=dtype,\n",
    "                                      device=device)\n",
    "            \n",
    "        # (batch, query_len, kv_len) -> (batch, att_head, query_len, kv_len``)\n",
    "        casual_mask = casual_mask.unsqueeze(1) # Adding a head dimensions since we will have multiple head in attention\n",
    "            \n",
    "        # ** get position of the tokens **\n",
    "        if kv_cache is not None and kv_cache.num_items() > 0:\n",
    "            position_ids = attention_mask.cumsum(-1)[:, -1]\n",
    "            if position_ids.dim() == 1:\n",
    "                position_ids = position_ids.unsqueeze(0)\n",
    "                \n",
    "        else:\n",
    "            # we are just giving the masked tokens a random position even though it's loss does not effect , it's just for compuation efficiency we can't leave it as is , so we are giving a random number\n",
    "            position_ids  = (attention_mask.cumsum(-1)[:, -1]).masked_fill_((attention_mask == 0), 1).to(device)\n",
    "            \n",
    "        def forward(\n",
    "            self,\n",
    "            input_ids:torch.LongTensor=None,\n",
    "            pixel_values:torch.FloatTensor=None,\n",
    "            attention_mask:Optional[torch.Tensor]=None,\n",
    "            kv_cache:Optional[KVCache]=None\n",
    "        ) -> Tuple:\n",
    "            assert torch.all(attention_mask==1), \"The input can't padded\" # for this implementation only\n",
    "            \n",
    "            pass\n",
    "    \n",
    "    def forward(self,\n",
    "                input_ids: torch.LongTensor = None,\n",
    "                pixel_values: torch.FloatTensor = None,\n",
    "                attention_mask: Optional[torch.Tensor] = None,\n",
    "                kv_cache: Optional[KVCache] = None) -> Tuple :\n",
    "        \n",
    "        assert torch.all(attention_mask == 1), \"The input can't be padded\"\n",
    "        \n",
    "        # tokens ids to embeddings [this input embedding contain embedding of image place holder token that need to replace by original image embedding]\n",
    "        input_embeddings = self.language_model.get_input_embeddings()(input_ids) # (B, Seq_len, embedding_size)\n",
    "        \n",
    "        # extract visual feature (b, patch_len, embedding )\n",
    "        visual_embeddings = self.vision_model(pixel_values)\n",
    "        \n",
    "        # project visual embedding to a standard embedding size as text embedding\n",
    "        visual_embeddings = self.multi_model_projector(visual_embeddings) # (b, patch_len, patch_embed) -> (b, patch_len, d_model) for merge between text and image embedding\n",
    "        \n",
    "        # replace image placeholder token embedding with real image embedding\n",
    "        input_embeddings, attention_mask, position_ids =  self._merge_input_ids_with_image_features(visual_embeddings, input_embeddings, input_ids, attention_mask, kv_cache)\n",
    "        \n",
    "        outputs = self.language_model(\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            input_embeddings=input_embeddings,\n",
    "            kv_cache=kv_cache\n",
    "        )\n",
    "        return outputs\n",
    "               \n",
    "        \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d11e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
