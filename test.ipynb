{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00f01490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2d0edb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check device\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297d02b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiglipVisionConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size=768,\n",
    "        intermediate_size=3072,\n",
    "        num_hidden_layers=12,\n",
    "        num_attention_heads=12,\n",
    "        num_channels=3,\n",
    "        image_size=224,\n",
    "        patch_size=16,\n",
    "        layer_norm_eps=1e-6,\n",
    "        attention_dropout=0.0,\n",
    "        num_image_tokens:int = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.num_channels = num_channels\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.num_image_tokens = num_image_tokens\n",
    "        \n",
    "        assert self.image_size**2 // self.patch_size**2 == 0, \"patchsize should be divisible with image size\"\n",
    "        \n",
    "config = SiglipVisionConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000a341b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiglipPatchEmbedding(nn.Module):\n",
    "    def __init__(self, config: SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patch_layer = nn.Conv2d(in_channels=3,\n",
    "                                     out_channels=config.hidden_size,\n",
    "                                     kernel_size=config.patch_size,\n",
    "                                     stride=config.patch_size,\n",
    "                                     padding='valid')\n",
    "        \n",
    "        # total number of pixel in image = 224*224, total pixel in a patch is 16*16 , so total patch number = 224*224 / 16*16\n",
    "        self.patch_num = config.image_size**2 // config.patch_size**2\n",
    "        \n",
    "        self.pos_embedding = nn.Embedding(num_embeddings=self.patch_num,\n",
    "                                          embedding_dim=config.hidden_size)\n",
    "        \n",
    "        self.register_buffer('position_ids',\n",
    "                             torch.arange(0, self.patch_num).view(1, -1),\n",
    "                             persistent=False) # `persistent` = do we need it as a part of module state dict\n",
    "        \n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.patch_layer(x) # (Batch , channel, height, width) => (Batch, hidden_size, height, width)\n",
    "        # x = torch.flatten(x, start_dim=-2, end_dim=-1) # [Batch, hidden_size, heigt*widht]\n",
    "        x = x.view(x.shape[0], x.shape[1], x.shape[2] * x.shape[3]) \n",
    "        x = x.transpose(-2, -1)\n",
    "        pos_embed = self.pos_embedding(self.position_ids)\n",
    "        return x + pos_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d115ee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiglipVisionTransformer(nn.Module):\n",
    "    def __init__(self, config: SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.patch_embedding = SiglipPatchEmbedding(config)\n",
    "        self.encoder = SiglipEncoder(config) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c1dd5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiglipVisionModel(nn.Module):\n",
    "    def __init__(self, config: SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.visual_model = SiglipVisionTransformer(config)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # [Batch, channel, height, width] -> [Batch, patch, embed_dim]\n",
    "        return self.visual_model(x)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ba537a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
