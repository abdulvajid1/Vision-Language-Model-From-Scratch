{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00f01490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2d0edb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check device\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "297d02b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiglipVisionConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size=768,\n",
    "        intermediate_size=3072,\n",
    "        num_hidden_layers=12,\n",
    "        num_attention_heads=12,\n",
    "        num_channels=3,\n",
    "        image_size=224,\n",
    "        patch_size=16,\n",
    "        layer_norm_eps=1e-6,\n",
    "        attention_dropout=0.0,\n",
    "        num_image_tokens:int = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.num_channels = num_channels\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.num_image_tokens = num_image_tokens\n",
    "        \n",
    "config = SiglipVisionConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "000a341b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiglipPatchEmbedding(nn.Module):\n",
    "    def __init__(self, config: SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patch_layer = nn.Conv2d(in_channels=3,\n",
    "                                     out_channels=config.hidden_size,\n",
    "                                     kernel_size=config.patch_size,\n",
    "                                     stride=config.patch_size,\n",
    "                                     padding='valid')\n",
    "        \n",
    "        # total number of pixel in image = 224*224, total pixel in a patch is 16*16 , so total patch number = 224*224 / 16*16\n",
    "        self.patch_num = config.image_size**2 // config.patch_size**2\n",
    "        \n",
    "        self.pos_embedding = nn.Embedding(num_embeddings=self.patch_num,\n",
    "                                          embedding_dim=config.hidden_size)\n",
    "        \n",
    "        self.register_buffer('position_ids',\n",
    "                             torch.arange(0, self.patch_num).view(1, -1),\n",
    "                             persistent=False) # `persistent` = do we need it as a part of module state dict\n",
    "        \n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.patch_layer(x) # (Batch , channel, height, width) => (Batch, hidden_size, height, width)\n",
    "        # x = torch.flatten(x, start_dim=-2, end_dim=-1) => [Batch, hidden_size, heigt*widht = num_patch]\n",
    "        x = x.view(x.shape[0], x.shape[1], x.shape[2] * x.shape[3]) \n",
    "        # (Batch, num_patch, hidden_size)\n",
    "        x = x.transpose(-2, -1)\n",
    "        x = x + self.pos_embedding(self.position_ids)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a52ce6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.eps = config.layer_norm_eps\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.scale = nn.Parameter(torch.ones(self.hidden_size))\n",
    "        self.shift = nn.Parameter(torch.zeros(self.hidden_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_mean = torch.mean(x, dim=-1, keepdim=True)\n",
    "        x_std = torch.std(x, dim=-1, keepdim=True)\n",
    "        x_norm = (x - x_mean) / (x_std + self.eps)\n",
    "        return x_norm * self.scale + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97cb9d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiglipEncoder(nn.Module):\n",
    "    def __init__(self, config: SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        self.eps = config.layer_norm_eps\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d115ee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiglipVisionTransformer(nn.Module):\n",
    "    def __init__(self, config: SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.patch_embedding = SiglipPatchEmbedding(config)\n",
    "        self.encoder = SiglipEncoder(config) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c1dd5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiglipVisionModel(nn.Module):\n",
    "    def __init__(self, config: SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.visual_model = SiglipVisionTransformer(config)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # [Batch, channel, height, width] -> [Batch, patch, embed_dim]\n",
    "        return self.visual_model(x)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e848efcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "x = torch.ones((5, 5, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e94a266",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
